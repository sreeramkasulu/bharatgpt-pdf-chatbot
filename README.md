# ğŸ’¬ BharatGPT â€” Your Indian Market Mentor ğŸ¤–  
**AI PDF Chatbot for Biomaterials, Sustainability, and Circular Economy in India**  
**Live Demo:** [https://bharatgpt-indian-market-mentor.streamlit.app/](https://bharatgpt-indian-market-mentor.streamlit.app/)

---

## ğŸ§  TECH STACK

- **FLAN-T5 Small (`MBZUAI/LaMini-Flan-T5-248M`)** â€“ Compact, fast, and accurate  
- **LangChain** â€“ Intelligent chaining and prompt handling  
- **FAISS** â€“ Vector similarity search over PDFs  
- **Flask** â€“ Lightweight web server  
- **HTML + CSS + JS** â€“ Front-end UI  
- **Runs on CPU** â€“ No GPU required  

---

## âœ… Fully Local â€” Works on Windows, macOS, Linux  
Requires **Python 3.9+** and `pip`

---

## ğŸ“ Project Setup

### â–¶ï¸ STEP 1: Setup Project Directory

1. Clone or unzip the project folder  
2. Add your PDF (e.g. `biomaterials_india.pdf`) to the root directory  
3. Open a terminal in the project folder  

---

### â–¶ï¸ STEP 2: Install Requirements & Start Server

```bash
pip install -r requirements.txt
python process_pdf.py     # Convert PDF to vectorstore
python app.py             # Launch the chatbot
```

Your browser will open at:  
ğŸ‘‰ http://localhost:5000

---

## ğŸ’¬ What Can You Ask?

Ask BharatGPT about:

- ğŸ“¦ Indiaâ€™s rules for biodegradable packaging  
- ğŸ­ Setting up a recycling facility with ULBs  
- ğŸ“Š Government schemes for sustainability startups  
- ğŸ§ª Partnering with IISc, CSIR, or BIRAC  
- ğŸ’¡ Examples like MYNUSCo, Recykal, or BioE3 Policy  

Out-of-context questions (e.g., "Who is Virat Kohli?") will return:  
> **"This question is out of context"**

---

## ğŸ§ª Test via `curl` (Optional)

```bash
curl -X POST http://localhost:5000/chat \
     -H "Content-Type: application/json" \
     -d "{\"message\": \"How to expand my startup in Indian market?\"}"
```

---

## ğŸ“Œ Features Recap

| Feature                             | Status     |
|-------------------------------------|------------|
| ğŸ§  PDFâ€‘trained LLM with FAISS        | âœ… Enabled |
| ğŸ“ Bulletâ€‘point answers (no repeats) | âœ… Clean   |
| ğŸš« Rejects outâ€‘of-context questions  | âœ… Working |
| âš¡ Fast inference (<3â€¯sec CPU)       | âœ… OK      |
| ğŸ“„ Shows helpful context snippets    | âœ… Smart   |

---

## ğŸ”’ Optional: Local `.gguf` LLM Support

To run a local LLM via `llama.cpp`, follow these steps:

1. Download the `.gguf` model file, e.g.:  
   `TinyLlama-1.1B-Chat-v1.0-GGUF` from Hugging Face  
2. Rename it to `model.gguf`  
3. Place it in the `/models/` directory  
4. Load it using `llama-cpp-python`, `Ollama`, or similar tools

---

## ğŸ‘¨â€ğŸ’» Author

**Made by:** Penke Sreeram Kasulu  
**Mission:** Help the world navigate the Indian sustainability market with AI ğŸŒ±
